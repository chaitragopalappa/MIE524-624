{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM3DJmYnKXuBx8UsrziAq3Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UHsf5Wq8ehBV"},"outputs":[],"source":["#!/usr/bin/env python3\n","#Lapan text book Chapter 6, example 1\n","import gym\n","import collections\n","#from gym.utils.play import play\n","#from tensorboardX import SummaryWriter\n","import numpy as np\n","\n","\n","ENV_NAME = \"FrozenLake-v1\"\n","TEST_EPISODES = 50"]},{"cell_type":"markdown","source":["\n","'''Things to try\n","1. Epsilon greedy action (set exploration_prob=0.1) v. random action selection policy (set exploration_prob = 1)\n","2. Learning rate\n","'''\n"],"metadata":{"id":"AXFBWSxHfRDR"}},{"cell_type":"code","source":["class QLearning:\n","    def __init__(self, num_states, num_actions, learning_rate=0.1, discount_factor=1, exploration_prob=1):\n","\n","\n","    def choose_action(self, state, random_action):\n","\n","    def update_q_table(self, state, action, next_state, reward):\n","\n","    def update_rates(self, iteration):\n","\n","    def play_episode(self, env): #play learned policy and return total reward\n","\n","    def get_q_table(self):\n",""],"metadata":{"id":"iLmTJmZcfAmx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    env = gym.make(ENV_NAME)\n","    #env =gym.make(ENV_NAME, render_mode=\"human\")\n","    ql_agent = QLearning(env.observation_space.n, env.action_space.n)\n","    #writer = SummaryWriter(comment=\"QL_frozen lake\")\n","    NUM_EPISODES = 10000\n","    best_reward = 0.0\n","    for episode in range(NUM_EPISODES):\n","        state = env.reset()\n","        done = False\n","\n","        while not done:\n","          FILL#Choose an action\n","          FILL # Take the chosen action and observe the next state and reward\n","          FILL #update Q-factors\n","\n","          state = next_state\n","\n","        # Evaluate the learned Q-table\n","        total_reward = 0\n","        for _ in range(TEST_EPISODES):\n","            total_reward += ql_agent.play_episode(env)\n","\n","        average_reward = total_reward / TEST_EPISODES\n","        #writer.add_scalar(\"reward\", average_reward, episode)\n","        if average_reward > best_reward:\n","            print(\"Best reward updated %.3f -> %.3f\" % (\n","                best_reward, average_reward))\n","            best_reward = average_reward\n","        if average_reward > 0.80:\n","            print(\"Solved in %d iterations!\" % episode)\n","            break\n","\n","    # Evaluate the learned Q-table\n","    #env = gym.wrappers.Monitor(env, \"recording\")\n","    q_table=ql_agent.get_q_table()\n","    print(q_table)\n","    #writer.close()\n","#tensorboard --logdir runs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3mDm-K4GfbKa","executionInfo":{"status":"ok","timestamp":1741047514481,"user_tz":300,"elapsed":9394,"user":{"displayName":"Chaitra Gopalappa","userId":"05469713652506822347"}},"outputId":"4e5b0f72-aaff-4037-8aec-20474d9481ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Best reward updated 0.000 -> 0.600\n","Best reward updated 0.600 -> 0.680\n","Best reward updated 0.680 -> 0.700\n","Best reward updated 0.700 -> 0.760\n","Best reward updated 0.760 -> 0.780\n","Best reward updated 0.780 -> 0.820\n","Solved in 752 iterations!\n","[[0.04271466 0.03947953 0.04066421 0.03817192]\n"," [0.02445388 0.02473359 0.02765454 0.03659854]\n"," [0.04007641 0.03221046 0.04595537 0.02180754]\n"," [0.01514859 0.01395877 0.00324801 0.01463811]\n"," [0.04559427 0.02557157 0.02368749 0.02652989]\n"," [0.         0.         0.         0.        ]\n"," [0.03748408 0.08034135 0.05540576 0.00509174]\n"," [0.         0.         0.         0.        ]\n"," [0.02616992 0.04477712 0.04982931 0.0575126 ]\n"," [0.04724975 0.10351466 0.07642008 0.06180346]\n"," [0.15716755 0.10813633 0.08199949 0.04677679]\n"," [0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.        ]\n"," [0.04844163 0.13608532 0.14128762 0.12683532]\n"," [0.1095055  0.38469979 0.34874781 0.35370361]\n"," [0.         0.         0.         0.        ]]\n"]}]}]}