{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UcJJ4VwRcHpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXxN4lv_c_ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DB4402pRNUJe",
        "outputId": "ddf03463-4352-488b-a36c-19a087293fa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.11/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Nov  8 14:06:40 2023\n",
        "@author: chaitrag\n",
        "\"\"\"\n",
        "import gym\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "ENV_NAME_2= \"CartPole-v1\"\n",
        "input_dim =4\n",
        "#ENV_NAME_3 = \"MountainCar-v0\"\n",
        "#input_dim=2\n",
        "\n",
        "env = gym.make(ENV_NAME_2)#, render_mode=\"human\")\n",
        "\n",
        "# Define hyperparameters\n",
        "gamma = 0.99\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.995\n",
        "learning_rate = 0.01\n",
        "batch_size = 64\n",
        "memory_size = 10000\n",
        "episode_length = 200\n",
        "target_update_frequency = 20  # Update the target network every N episodes\n",
        "\n",
        "# Define the Q-network\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create both the main and target networks\n",
        "input_dim = input_dim\n",
        "output_dim = env.action_space.n\n",
        "main_network = QNetwork(input_dim, output_dim)\n",
        "target_network = QNetwork(input_dim, output_dim)\n",
        "target_network.load_state_dict(main_network.state_dict())  # Initialize target network with main network's weights\n",
        "optimizer = optim.Adam(main_network.parameters(), lr=learning_rate)\n",
        "\n",
        "# Initialize replay memory\n",
        "replay_memory = []\n",
        "\n",
        "# Function to choose an action\n",
        "def choose_action(state):\n",
        "    if np.random.rand() <= epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        q_values = main_network(torch.Tensor(state))\n",
        "        return torch.argmax(q_values).item()\n",
        "def get_Q_network_output(state):\n",
        "    return main_network(torch.Tensor(state))"
      ],
      "metadata": {
        "id": "xUMLqxV8ijWb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main training loop\n",
        "num_episodes = 100\n",
        "writer = SummaryWriter()\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    for t in range(episode_length):\n",
        "        action = choose_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        replay_memory.append((state, action, reward, next_state, done))#store experiences into replay memory\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            #print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
        "            writer.add_scalar(\"Loss/train\", loss, episode)\n",
        "            writer.add_scalar(\"Total reward\", total_reward,episode)\n",
        "            writer.add_scalar(\"epsilon\", epsilon, episode)\n",
        "            break\n",
        "\n",
        "        if len(replay_memory) >= batch_size:\n",
        "            batch = random.sample(replay_memory, batch_size)\n",
        "\n",
        "            for state_batch, action_batch, reward_batch, next_state_batch, done_batch in batch:\n",
        "                state_batch = torch.Tensor(state_batch)\n",
        "                next_state_batch = torch.Tensor(next_state_batch)\n",
        "\n",
        "                q_values = main_network(state_batch)\n",
        "                target = q_values.clone().detach()\n",
        "\n",
        "                if not done_batch:\n",
        "                    target[action_batch] = reward_batch + gamma * torch.max(target_network(next_state_batch))\n",
        "                else:\n",
        "                    target[action_batch] = reward_batch\n",
        "\n",
        "                loss = nn.MSELoss()(q_values, target)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            # Decay exploration rate\n",
        "            if epsilon > epsilon_min:\n",
        "               epsilon *= epsilon_decay\n",
        "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
        "\n",
        "    # Update the target network\n",
        "    if episode % target_update_frequency == 0:\n",
        "        target_network.load_state_dict(main_network.state_dict())\n",
        "writer.close()\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4RcpUaWizwe",
        "outputId": "fd5bf8c0-0d80-41c7-c1f3-88f9b3e3e79a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1, Total Reward: 16.0, Epsilon: 0.990025\n",
            "Episode: 2, Total Reward: 14.0, Epsilon: 0.9275689688183278\n",
            "Episode: 3, Total Reward: 9.0, Epsilon: 0.8911090557802088\n",
            "Episode: 4, Total Reward: 24.0, Epsilon: 0.7940753492934954\n",
            "Episode: 5, Total Reward: 21.0, Epsilon: 0.7183288830986236\n",
            "Episode: 6, Total Reward: 14.0, Epsilon: 0.6730128848950395\n",
            "Episode: 7, Total Reward: 9.0, Epsilon: 0.6465587967553006\n",
            "Episode: 8, Total Reward: 16.0, Epsilon: 0.5997278763867329\n",
            "Episode: 9, Total Reward: 22.0, Epsilon: 0.5398075216808175\n",
            "Episode: 10, Total Reward: 10.0, Epsilon: 0.5159963842937159\n",
            "Episode: 11, Total Reward: 14.0, Epsilon: 0.483444593917636\n",
            "Episode: 12, Total Reward: 14.0, Epsilon: 0.4529463432347434\n",
            "Episode: 13, Total Reward: 18.0, Epsilon: 0.4159480862733536\n",
            "Episode: 14, Total Reward: 11.0, Epsilon: 0.39561243860243744\n",
            "Episode: 15, Total Reward: 38.0, Epsilon: 0.32864265128599696\n",
            "Episode: 16, Total Reward: 12.0, Epsilon: 0.31101247816653554\n",
            "Episode: 17, Total Reward: 15.0, Epsilon: 0.28993519966087045\n",
            "Episode: 18, Total Reward: 9.0, Epsilon: 0.27853872940185365\n",
            "Episode: 19, Total Reward: 9.0, Epsilon: 0.26759021970270175\n",
            "Episode: 20, Total Reward: 8.0, Epsilon: 0.2583638820072446\n",
            "Episode: 21, Total Reward: 9.0, Epsilon: 0.24820838415550486\n",
            "Episode: 22, Total Reward: 9.0, Epsilon: 0.2384520680152932\n",
            "Episode: 23, Total Reward: 12.0, Epsilon: 0.22566020663225933\n",
            "Episode: 24, Total Reward: 11.0, Epsilon: 0.21462770857094118\n",
            "Episode: 25, Total Reward: 10.0, Epsilon: 0.20516038984972615\n",
            "Episode: 26, Total Reward: 16.0, Epsilon: 0.1903004112552766\n",
            "Episode: 27, Total Reward: 15.0, Epsilon: 0.17740377510930716\n",
            "Episode: 28, Total Reward: 10.0, Epsilon: 0.16957841978827493\n",
            "Episode: 29, Total Reward: 10.0, Epsilon: 0.16209824418995536\n",
            "Episode: 30, Total Reward: 10.0, Epsilon: 0.1549480222912372\n",
            "Episode: 31, Total Reward: 13.0, Epsilon: 0.14590259167570602\n",
            "Episode: 32, Total Reward: 10.0, Epsilon: 0.13946676683816583\n",
            "Episode: 33, Total Reward: 9.0, Epsilon: 0.13398475271138335\n",
            "Episode: 34, Total Reward: 9.0, Epsilon: 0.12871821987500112\n",
            "Episode: 35, Total Reward: 9.0, Epsilon: 0.12365869841532712\n",
            "Episode: 36, Total Reward: 10.0, Epsilon: 0.11820406108847166\n",
            "Episode: 37, Total Reward: 9.0, Epsilon: 0.1135578192100607\n",
            "Episode: 38, Total Reward: 12.0, Epsilon: 0.10746596228306791\n",
            "Episode: 39, Total Reward: 10.0, Epsilon: 0.10272559337455119\n",
            "Episode: 40, Total Reward: 11.0, Epsilon: 0.09770335251664321\n",
            "Episode: 41, Total Reward: 11.0, Epsilon: 0.09292664835904782\n",
            "Episode: 42, Total Reward: 13.0, Epsilon: 0.08750185146499175\n",
            "Episode: 43, Total Reward: 13.0, Epsilon: 0.08239373898667031\n",
            "Episode: 44, Total Reward: 11.0, Epsilon: 0.07836551983717477\n",
            "Episode: 45, Total Reward: 11.0, Epsilon: 0.0745342397963533\n",
            "Episode: 46, Total Reward: 9.0, Epsilon: 0.0716045256805401\n",
            "Episode: 47, Total Reward: 11.0, Epsilon: 0.06810378976195819\n",
            "Episode: 48, Total Reward: 10.0, Epsilon: 0.06509970288011008\n",
            "Episode: 49, Total Reward: 10.0, Epsilon: 0.062228127537270826\n",
            "Episode: 50, Total Reward: 11.0, Epsilon: 0.05918580250061433\n",
            "Episode: 51, Total Reward: 9.0, Epsilon: 0.05685938874076987\n",
            "Episode: 52, Total Reward: 8.0, Epsilon: 0.05489891379418004\n",
            "Episode: 53, Total Reward: 9.0, Epsilon: 0.052741004581916356\n",
            "Episode: 54, Total Reward: 9.0, Epsilon: 0.05066791621302729\n",
            "Episode: 55, Total Reward: 8.0, Epsilon: 0.04892091923449087\n",
            "Episode: 56, Total Reward: 9.0, Epsilon: 0.046997986793891174\n",
            "Episode: 57, Total Reward: 10.0, Epsilon: 0.044924885780066794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "S1ZKSttyhZRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test - with vizualization\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "from pyvirtualdisplay import Display\n",
        "Display().start()\n",
        "\n",
        "epsilon = 0\n",
        "TEST_EPISODES=2\n",
        "total_reward = 0\n",
        "img = plt.imshow(env.render('rgb_array')) # only call this once\n",
        "for test_episode in range(TEST_EPISODES):\n",
        "    state = env.reset(seed=77)\n",
        "    while True:\n",
        "        img.set_data(env.render('rgb_array')) # just update the data\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "        action = choose_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            #print(f\"Episode: {test_episode + 1}, Total Reward: {total_reward}, Epsilon: {epsilon}\")\n",
        "            break\n",
        "average_reward = total_reward / TEST_EPISODES\n",
        "print(f\"Total reward:{average_reward} \")\n",
        "if average_reward > 200:\n",
        "    print (\"Good job: Total reward exceeded threshold of 500\")\n",
        "else:\n",
        "    print (\"You can do better: Total reward did not exceed threshold of 500\")\n",
        "#Threshold for reward for cart_pole_v1 500\n",
        "env.close()"
      ],
      "metadata": {
        "id": "9o4X5SmANyh7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}